import fs from 'fs';
import path from 'path';
import { fileURLToPath } from 'url';
import { createRequire } from 'module';

// Use require for CommonJS module (pdf-parse is CommonJS only)
const require = createRequire(import.meta.url);
const pdfParse = require('pdf-parse');

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// Simple tokenizer (approximates GPT tokens: ~4 chars per token)
function estimateTokens(text) {
  return Math.ceil(text.length / 4);
}

// Chunk text into pieces of approximately maxTokens
function chunkText(text, maxTokens = 500) {
  const chunks = [];
  const paragraphs = text.split(/\n\n+/);

  let currentChunk = '';
  let currentTokens = 0;

  for (const paragraph of paragraphs) {
    const paragraphTokens = estimateTokens(paragraph);

    if (currentTokens + paragraphTokens > maxTokens && currentChunk) {
      chunks.push(currentChunk.trim());
      currentChunk = paragraph;
      currentTokens = paragraphTokens;
    } else {
      currentChunk += (currentChunk ? '\n\n' : '') + paragraph;
      currentTokens += paragraphTokens;
    }
  }

  if (currentChunk.trim()) {
    chunks.push(currentChunk.trim());
  }

  return chunks;
}

// Extract section headers from text
function extractSectionHeader(text, previousHeaders = []) {
  // Match common patterns like "Chapter 1", "Section A", etc.
  const headerPatterns = [
    /^(Chapter\s+\d+[:\s]+.+?)(?:\n|$)/mi,
    /^(Section\s+[A-Z0-9]+[:\s]+.+?)(?:\n|$)/mi,
    /^([A-Z][A-Z\s]{3,}?)(?:\n|$)/m, // ALL CAPS headers
    /^(\d+\.\s+[A-Z].+?)(?:\n|$)/m, // Numbered sections
  ];

  for (const pattern of headerPatterns) {
    const match = text.match(pattern);
    if (match) {
      return match[1].trim();
    }
  }

  return previousHeaders.length > 0 ? previousHeaders[previousHeaders.length - 1] : 'Introduction';
}

// Escape SQL strings
function escapeSql(str) {
  if (!str) return '';
  return str.replace(/'/g, "''");
}

async function generateSqlFile(pdfPath, outputPath) {
  console.log(`\nüìÑ Reading PDF: ${pdfPath}`);

  // Read PDF file
  const dataBuffer = fs.readFileSync(pdfPath);
  const pdfData = await pdfParse(dataBuffer);

  console.log(`‚úÖ PDF parsed successfully`);
  console.log(`   Pages: ${pdfData.numpages}`);
  console.log(`   Total text length: ${pdfData.text.length} characters`);

  // Extract basic info
  const title = '2024 CSDA Attorney Sourcebook';
  const source = 'CSDA Sourcebook';
  const source_url = null;

  // Chunk the text
  console.log(`\n‚úÇÔ∏è  Chunking text into ~500 token pieces...`);
  const textChunks = chunkText(pdfData.text, 500);
  console.log(`   Created ${textChunks.length} chunks`);

  // Generate SQL
  console.log(`\nüìù Generating SQL file...`);

  let sql = `-- Auto-generated SQL seed file for 2024 CSDA Attorney Sourcebook
-- Generated: ${new Date().toISOString()}
-- Total chunks: ${textChunks.length}

-- Insert document record
INSERT INTO documents (id, title, source, source_url, section, status, created_at, updated_at)
VALUES (
  '00000000-0000-0000-0000-000000000001'::uuid,
  '${escapeSql(title)}',
  '${escapeSql(source)}',
  ${source_url ? `'${escapeSql(source_url)}'` : 'NULL'},
  'Full Document',
  'completed',
  NOW(),
  NOW()
);

-- Insert chunks with search vectors (will be auto-generated by trigger)
`;

  let currentSection = 'Introduction';

  for (let i = 0; i < textChunks.length; i++) {
    const chunk = textChunks[i];

    // Try to extract section header from chunk
    const sectionHeader = extractSectionHeader(chunk, [currentSection]);
    if (sectionHeader !== currentSection) {
      currentSection = sectionHeader;
      console.log(`   Found section: ${currentSection}`);
    }

    // Escape content for SQL
    const escapedContent = escapeSql(chunk);
    const escapedSection = escapeSql(currentSection);

    sql += `
INSERT INTO chunks (document_id, content, section_title, chunk_index)
VALUES (
  '00000000-0000-0000-0000-000000000001'::uuid,
  '${escapedContent}',
  '${escapedSection}',
  ${i}
);
`;

    // Progress indicator
    if ((i + 1) % 50 === 0) {
      console.log(`   Progress: ${i + 1}/${textChunks.length} chunks processed`);
    }
  }

  // Write to file
  fs.writeFileSync(outputPath, sql);
  console.log(`\n‚úÖ SQL file generated: ${outputPath}`);

  // Summary
  console.log(`\nüìä Summary:`);
  console.log(`   Document: ${title}`);
  console.log(`   Source: ${source}`);
  console.log(`   Total Chunks: ${textChunks.length}`);
  console.log(`   SQL File Size: ${Math.round(fs.statSync(outputPath).size / 1024)} KB`);
  console.log(`\nüí° To import:`);
  console.log(`   1. Go to Neon SQL Editor`);
  console.log(`   2. Copy and paste the contents of ${outputPath}`);
  console.log(`   3. Execute the SQL`);

  return {
    chunksCreated: textChunks.length,
    title,
    source,
    outputPath
  };
}

// Main execution
const pdfPath = process.argv[2];
const outputPath = process.argv[3] || 'db/seed-sourcebook.sql';

if (!pdfPath) {
  console.error('‚ùå Error: Please provide a PDF file path');
  console.error('Usage: node scripts/generate-seed-sql.js <path-to-pdf> [output-sql-file]');
  process.exit(1);
}

if (!fs.existsSync(pdfPath)) {
  console.error(`‚ùå Error: PDF file not found: ${pdfPath}`);
  process.exit(1);
}

generateSqlFile(pdfPath, outputPath)
  .then((result) => {
    console.log(`\n‚ú® SQL generation completed successfully!`);
    process.exit(0);
  })
  .catch((error) => {
    console.error('\n‚ùå Error during SQL generation:', error);
    process.exit(1);
  });
