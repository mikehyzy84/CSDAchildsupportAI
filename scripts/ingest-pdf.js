import fs from 'fs';
import path from 'path';
import { fileURLToPath } from 'url';
import { neon } from '@neondatabase/serverless';
import dotenv from 'dotenv';
import { createRequire } from 'module';

// Use require for CommonJS module (pdf-parse is CommonJS only)
const require = createRequire(import.meta.url);
const pdfParse = require('pdf-parse');

// Load environment variables
dotenv.config();

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// Initialize database connection
const sql = neon(process.env.DATABASE_URL);

// Simple tokenizer (approximates GPT tokens: ~4 chars per token)
function estimateTokens(text) {
  return Math.ceil(text.length / 4);
}

// Chunk text into pieces of approximately maxTokens
function chunkText(text, maxTokens = 500) {
  const chunks = [];
  const paragraphs = text.split(/\n\n+/);

  let currentChunk = '';
  let currentTokens = 0;

  for (const paragraph of paragraphs) {
    const paragraphTokens = estimateTokens(paragraph);

    if (currentTokens + paragraphTokens > maxTokens && currentChunk) {
      chunks.push(currentChunk.trim());
      currentChunk = paragraph;
      currentTokens = paragraphTokens;
    } else {
      currentChunk += (currentChunk ? '\n\n' : '') + paragraph;
      currentTokens += paragraphTokens;
    }
  }

  if (currentChunk.trim()) {
    chunks.push(currentChunk.trim());
  }

  return chunks;
}

// Extract section headers from text
function extractSectionHeader(text, previousHeaders = []) {
  // Match common patterns like "Chapter 1", "Section A", etc.
  const headerPatterns = [
    /^(Chapter\s+\d+[:\s]+.+?)(?:\n|$)/mi,
    /^(Section\s+[A-Z0-9]+[:\s]+.+?)(?:\n|$)/mi,
    /^([A-Z][A-Z\s]{3,}?)(?:\n|$)/m, // ALL CAPS headers
    /^(\d+\.\s+[A-Z].+?)(?:\n|$)/m, // Numbered sections
  ];

  for (const pattern of headerPatterns) {
    const match = text.match(pattern);
    if (match) {
      return match[1].trim();
    }
  }

  // If no header found, use the last known header
  return previousHeaders.length > 0 ? previousHeaders[previousHeaders.length - 1] : 'Introduction';
}

async function ingestPDF(pdfPath) {
  console.log(`\nüìÑ Reading PDF: ${pdfPath}`);

  // Read PDF file
  const dataBuffer = fs.readFileSync(pdfPath);
  const pdfData = await pdfParse(dataBuffer);

  console.log(`‚úÖ PDF parsed successfully`);
  console.log(`   Pages: ${pdfData.numpages}`);
  console.log(`   Total text length: ${pdfData.text.length} characters`);

  // Extract basic info
  const filename = path.basename(pdfPath);
  const title = '2024 CSDA Attorney Sourcebook';
  const source = 'CSDA Sourcebook';
  const source_url = null;

  // Chunk the text
  console.log(`\n‚úÇÔ∏è  Chunking text into ~500 token pieces...`);
  const textChunks = chunkText(pdfData.text, 500);
  console.log(`   Created ${textChunks.length} chunks`);

  // Insert document record
  console.log(`\nüíæ Inserting document record...`);
  const documentResult = await sql`
    INSERT INTO documents (title, source, source_url, section, status)
    VALUES (${title}, ${source}, ${source_url}, 'Full Document', 'completed')
    RETURNING id
  `;

  const documentId = documentResult[0].id;
  console.log(`   Document ID: ${documentId}`);

  // Insert chunks with section headers
  console.log(`\nüì¶ Inserting chunks into database...`);
  let insertedCount = 0;
  let currentSection = 'Introduction';

  for (let i = 0; i < textChunks.length; i++) {
    const chunk = textChunks[i];

    // Try to extract section header from chunk
    const sectionHeader = extractSectionHeader(chunk, [currentSection]);
    if (sectionHeader !== currentSection) {
      currentSection = sectionHeader;
      console.log(`   Found section: ${currentSection}`);
    }

    // Insert chunk (the trigger will auto-generate search_vector)
    await sql`
      INSERT INTO chunks (document_id, content, section_title, chunk_index)
      VALUES (${documentId}, ${chunk}, ${currentSection}, ${i})
    `;

    insertedCount++;

    // Progress indicator
    if ((i + 1) % 50 === 0) {
      console.log(`   Progress: ${i + 1}/${textChunks.length} chunks inserted`);
    }
  }

  console.log(`   ‚úÖ All ${insertedCount} chunks inserted successfully`);

  // Summary
  console.log(`\nüìä Summary:`);
  console.log(`   Document: ${title}`);
  console.log(`   Source: ${source}`);
  console.log(`   Document ID: ${documentId}`);
  console.log(`   Total Chunks: ${insertedCount}`);
  console.log(`   Status: completed`);
  console.log(`   Search vectors: auto-generated by database trigger`);

  return {
    documentId,
    chunksCreated: insertedCount,
    title,
    source
  };
}

// Main execution
const pdfPath = process.argv[2];

if (!pdfPath) {
  console.error('‚ùå Error: Please provide a PDF file path');
  console.error('Usage: node scripts/ingest-pdf.js <path-to-pdf>');
  process.exit(1);
}

if (!fs.existsSync(pdfPath)) {
  console.error(`‚ùå Error: PDF file not found: ${pdfPath}`);
  process.exit(1);
}

ingestPDF(pdfPath)
  .then((result) => {
    console.log(`\n‚ú® PDF ingestion completed successfully!`);
    process.exit(0);
  })
  .catch((error) => {
    console.error('\n‚ùå Error during PDF ingestion:', error);
    process.exit(1);
  });
